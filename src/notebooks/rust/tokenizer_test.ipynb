{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "123956f0-5ce8-4fbd-a665-49e1797743c1",
   "metadata": {},
   "outputs": [],
   "source": [
    ":dep ndarray = {version=\"0.16.1\",features=[\"rayon\"]}\n",
    ":dep tokenizers = \"0.21.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7585cd9e-e7f0-49d5-967b-2081b2ce1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "use std::time::Instant;\n",
    "use tokenizers::{\n",
    "    utils::{\n",
    "        padding::{PaddingParams,PaddingStrategy},\n",
    "        truncation::{TruncationParams,TruncationStrategy}\n",
    "    },\n",
    "    models::wordpiece::WordPiece,\n",
    "    tokenizer::{Encoding,Result},\n",
    "    Tokenizer\n",
    "};\n",
    "use ndarray::{\n",
    "    aview1,\n",
    "    Array2,\n",
    "    Axis,\n",
    "    parallel::{\n",
    "        par_azip\n",
    "    }\n",
    "};\n",
    "\n",
    "#[derive(Debug, Clone, PartialEq)]\n",
    "pub struct BatchSentenceEncoding {\n",
    "    pub input_ids: Array2<u32>,\n",
    "    pub token_type_ids: Array2<u32>,\n",
    "    pub attention_mask: Array2<u32>\n",
    "}\n",
    "\n",
    "unsafe impl Send for  BatchSentenceEncoding {}\n",
    "unsafe impl Sync for BatchSentenceEncoding {}\n",
    "\n",
    "impl BatchSentenceEncoding {\n",
    "    pub fn empty(nrows: usize,ncols: usize) -> Self {\n",
    "        Self {\n",
    "            input_ids: Array2::<u32>::zeros((nrows,ncols)),\n",
    "            token_type_ids: Array2::<u32>::zeros((nrows,ncols)),\n",
    "            attention_mask: Array2::<u32>::zeros((nrows,ncols))\n",
    "        }\n",
    "    }\n",
    "\n",
    "    pub fn from_hf_batch_par(batch: Vec<Encoding>) -> Result<Self> {\n",
    "        let nrows = batch.len();\n",
    "        let ncols = batch[0].get_ids().len();\n",
    "        let mut batch_encodings = Self::empty(nrows,ncols);\n",
    "        par_azip!(\n",
    "            (\n",
    "                mut input_chunk in batch_encodings.input_ids.rows_mut(),\n",
    "                mut type_chunk in batch_encodings.token_type_ids.rows_mut(),\n",
    "                mut mask_chunk in batch_encodings.attention_mask.rows_mut(),\n",
    "                encoding in &batch\n",
    "            )\n",
    "            {\n",
    "                input_chunk.assign(&aview1(encoding.get_ids()));\n",
    "                type_chunk.assign(&aview1(encoding.get_type_ids()));\n",
    "                mask_chunk.assign(&aview1(encoding.get_attention_mask()));\n",
    "            }\n",
    "        );\n",
    "        Ok(batch_encodings)\n",
    "    }\n",
    "\n",
    "    pub fn from_hf_batch_serial(batch: Vec<Encoding>) -> Result<Self> {\n",
    "        let nrows = batch.len();\n",
    "        let ncols = batch[0].get_ids().len();\n",
    "        let mut batch_encodings = Self::empty(nrows,ncols);\n",
    "        batch.iter().enumerate().for_each(|(i,encoding)| {\n",
    "            batch_encodings.input_ids.row_mut(i).assign(&aview1(encoding.get_ids()));\n",
    "            batch_encodings.token_type_ids.row_mut(i).assign(&aview1(encoding.get_type_ids()));\n",
    "            batch_encodings.attention_mask.row_mut(i).assign(&aview1(encoding.get_attention_mask()));\n",
    "        });\n",
    "        Ok(batch_encodings)\n",
    "    }\n",
    "\n",
    "    pub fn from_hf_batch(batch: Vec<Encoding>, par_thresh: usize) -> Result<Self> {\n",
    "        let nrows = batch.len();\n",
    "        if nrows >= par_thresh {\n",
    "            Self::from_hf_batch_par(batch)\n",
    "        } else {\n",
    "            Self::from_hf_batch_serial(batch)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "let mut padding_params = PaddingParams::default();\n",
    "padding_params.strategy = PaddingStrategy::BatchLongest;\n",
    "let mut truncation_params = TruncationParams::default();\n",
    "truncation_params.strategy = TruncationStrategy::LongestFirst;\n",
    "let mut tokenizer = Tokenizer::from_file(\"tokenizer.json\")?;\n",
    "tokenizer.with_padding(Some(padding_params)).with_truncation(Some(truncation_params))?;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5fea8a4-2b96-458d-a4fd-e4dd5384ff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial Elapsed: 18\n",
      "BatchSentenceEncoding { input_ids: [[2002, 16193, 9849, 1999, 13918, 2943, 2974, 1012, 0, 0, 0, 0, 0],\n",
      " [2010, 21644, 2081, 5943, 2373, 2062, 7801, 1012, 0, 0, 0, 0, 0],\n",
      " [2982, 1998, 5038, 2628, 2010, 23222, 2147, 1012, 0, 0, 0, 0, 0],\n",
      " [2010, 8027, 4427, 2925, 8213, 1997, 6529, 1012, 0, 0, 0, 0, 0],\n",
      " [2027, 4114, 1037, 2782, 12964, 3451, 8906, 1012, 0, 0, 0, 0, 0],\n",
      " [2189, 1010, 3153, 1010, 1998, 12846, 2013, 2105, 1996, 2088, 2020, 2956, 1012],\n",
      " [1996, 2724, 6469, 2098, 4824, 1998, 12284, 2426, 19973, 1012, 0, 0, 0],\n",
      " [2009, 2150, 2019, 3296, 4535, 24188, 13295, 2011, 1996, 2451, 1012, 0, 0],\n",
      " [10660, 8346, 1999, 9871, 5301, 5776, 13105, 1012, 0, 0, 0, 0, 0]], shape=[9, 13], strides=[13, 1], layout=Cc (0x5), const ndim=2, token_type_ids: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], shape=[9, 13], strides=[13, 1], layout=Cc (0x5), const ndim=2, attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]], shape=[9, 13], strides=[13, 1], layout=Cc (0x5), const ndim=2 }\n"
     ]
    }
   ],
   "source": [
    "let batch = vec![\n",
    "    \"He pioneered advances in renewable energy technology.\",\n",
    "    \"His inventions made solar power more accessible.\",\n",
    "    \"Awards and recognition followed his groundbreaking work.\",\n",
    "    \"His legacy inspired future generations of scientists.\",\n",
    "    \"They organized a festival celebrating cultural diversity.\",\n",
    "    \"Music, dance, and cuisine from around the world were featured.\",\n",
    "    \"The event fostered understanding and appreciation among attendees.\",\n",
    "    \"It became an annual tradition cherished by the community.\",\n",
    "    \"Technological integration in healthcare improved patient outcomes.\",\n",
    "    // \"Electronic records streamlined information sharing.\",\n",
    "    // \"Telemedicine expanded access to remote areas.\",\n",
    "    // \"Data analytics guided preventative care measures.\"\n",
    "];\n",
    "let encoded_batch = tokenizer.encode_batch(batch,false)?;\n",
    "let start = Instant::now();\n",
    "let batch_encodings = BatchSentenceEncoding::from_hf_batch_serial(encoded_batch)?;\n",
    "println!(\"Serial Elapsed: {}\",start.elapsed().as_micros());\n",
    "println!(\"{:?}\",batch_encodings);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f536e20-59c6-4a16-9a45-da5f69852b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel Elapsed: 8\n",
      "BatchSentenceEncoding { input_ids: [[2002, 16193, 9849, 1999, 13918, 2943, 2974, 1012, 0, 0, 0, 0, 0],\n",
      " [2010, 21644, 2081, 5943, 2373, 2062, 7801, 1012, 0, 0, 0, 0, 0],\n",
      " [2982, 1998, 5038, 2628, 2010, 23222, 2147, 1012, 0, 0, 0, 0, 0],\n",
      " [2010, 8027, 4427, 2925, 8213, 1997, 6529, 1012, 0, 0, 0, 0, 0],\n",
      " [2027, 4114, 1037, 2782, 12964, 3451, 8906, 1012, 0, 0, 0, 0, 0],\n",
      " [2189, 1010, 3153, 1010, 1998, 12846, 2013, 2105, 1996, 2088, 2020, 2956, 1012],\n",
      " [1996, 2724, 6469, 2098, 4824, 1998, 12284, 2426, 19973, 1012, 0, 0, 0],\n",
      " [2009, 2150, 2019, 3296, 4535, 24188, 13295, 2011, 1996, 2451, 1012, 0, 0],\n",
      " [10660, 8346, 1999, 9871, 5301, 5776, 13105, 1012, 0, 0, 0, 0, 0]], shape=[9, 13], strides=[13, 1], layout=Cc (0x5), const ndim=2, token_type_ids: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], shape=[9, 13], strides=[13, 1], layout=Cc (0x5), const ndim=2, attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]], shape=[9, 13], strides=[13, 1], layout=Cc (0x5), const ndim=2 }\n"
     ]
    }
   ],
   "source": [
    "let batch = vec![\n",
    "    \"He pioneered advances in renewable energy technology.\",\n",
    "    \"His inventions made solar power more accessible.\",\n",
    "    \"Awards and recognition followed his groundbreaking work.\",\n",
    "    \"His legacy inspired future generations of scientists.\",\n",
    "    \"They organized a festival celebrating cultural diversity.\",\n",
    "    \"Music, dance, and cuisine from around the world were featured.\",\n",
    "    \"The event fostered understanding and appreciation among attendees.\",\n",
    "    \"It became an annual tradition cherished by the community.\",\n",
    "    \"Technological integration in healthcare improved patient outcomes.\",\n",
    "    // \"Electronic records streamlined information sharing.\",\n",
    "    // \"Telemedicine expanded access to remote areas.\",\n",
    "    // \"Data analytics guided preventative care measures.\"\n",
    "];\n",
    "let encoded_batch = tokenizer.encode_batch(batch,false)?;\n",
    "let start = Instant::now();\n",
    "let batch_encodings = BatchSentenceEncoding::from_hf_batch_serial(encoded_batch)?;\n",
    "println!(\"Parallel Elapsed: {}\",start.elapsed().as_micros());\n",
    "println!(\"{:?}\",batch_encodings);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df00134-7094-4ad0-afaa-b5273f4ddeb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rust",
   "language": "rust",
   "name": "rust"
  },
  "language_info": {
   "codemirror_mode": "rust",
   "file_extension": ".rs",
   "mimetype": "text/rust",
   "name": "Rust",
   "pygment_lexer": "rust",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
